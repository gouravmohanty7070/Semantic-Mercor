{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"9c8ad77c-589e-4228-a360-8656a4cc01dd","_cell_guid":"221d85f9-f546-4122-98d2-6bb2d6c9fca6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\n\n# 'datasets' is a package that provides easy access to many popular datasets for natural language processing (NLP) tasks.","metadata":{"_uuid":"c33d806e-1dc2-4f95-a134-ca71e7f600bc","_cell_guid":"de878f7e-3be1-4feb-9937-f90c5cbbb06e","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:01:38.156971Z","iopub.execute_input":"2023-05-10T10:01:38.157384Z","iopub.status.idle":"2023-05-10T10:01:49.644910Z","shell.execute_reply.started":"2023-05-10T10:01:38.157346Z","shell.execute_reply":"2023-05-10T10:01:49.643721Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pinecone-client\n\n# 'pinecone-client' is a Python library that provides a client for Pinecone, a cloud-based vector database service. ","metadata":{"_uuid":"e4c83d5a-ae54-4098-a21f-9d445dfd8b29","_cell_guid":"5c5761ed-7e84-40db-be1c-b9f14086f5a6","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:01:52.822815Z","iopub.execute_input":"2023-05-10T10:01:52.823206Z","iopub.status.idle":"2023-05-10T10:02:03.951217Z","shell.execute_reply.started":"2023-05-10T10:01:52.823163Z","shell.execute_reply":"2023-05-10T10:02:03.950072Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install sentence-transformers\n\n# 'sentence-transformers' is a Python package that provides pre-trained models for generating fixed-length dense embeddings for sentences or paragraphs. These embeddings can be used to perform similarity searches, clustering, or other NLP tasks.","metadata":{"_uuid":"6348b03d-69f3-407e-9a73-0b17a227adff","_cell_guid":"cce03e55-fe7c-4453-bb4e-9d3dfdbc5124","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:02:05.558504Z","iopub.execute_input":"2023-05-10T10:02:05.559579Z","iopub.status.idle":"2023-05-10T10:02:17.990995Z","shell.execute_reply.started":"2023-05-10T10:02:05.559529Z","shell.execute_reply":"2023-05-10T10:02:17.989706Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch\n\n# 'torch' is a Python package that provides support for tensor computations and deep learning. It is widely used in the machine learning community for building neural networks and training models.","metadata":{"_uuid":"5107cf4c-d277-4f6d-8d9c-7d8ac7229c0c","_cell_guid":"78349f40-3f45-44dd-878d-1af0c8cb3ac4","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:02:20.939565Z","iopub.execute_input":"2023-05-10T10:02:20.939961Z","iopub.status.idle":"2023-05-10T10:02:31.855204Z","shell.execute_reply.started":"2023-05-10T10:02:20.939928Z","shell.execute_reply":"2023-05-10T10:02:31.853798Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndf = load_dataset(\"squad\", split=\"train\").to_pandas()\ndf = df[[\"title\", \"context\"]]\ndf = df.drop_duplicates(subset=\"context\")\ndf\n\n#This code loads the squad dataset using the load_dataset function from the datasets package. The split=\"train\" argument specifies that we want to load the training split of the dataset.\n\n#The loaded dataset is then converted to a Pandas DataFrame using the .to_pandas() method. The resulting DataFrame contains two columns: \"title\" and \"context\".\n\n#The next line of code removes any duplicate rows in the DataFrame, based on the values in the \"context\" column. This is done using the .drop_duplicates() method.\n\n#Finally, the cleaned DataFrame is assigned back to the variable df.","metadata":{"_uuid":"109b5a2d-3ff8-4123-b5c6-9dffdeadbc82","_cell_guid":"1f35ed34-20d9-478e-840e-aeff5d1c2f45","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:02:32.932495Z","iopub.execute_input":"2023-05-10T10:02:32.932877Z","iopub.status.idle":"2023-05-10T10:02:34.587373Z","shell.execute_reply.started":"2023-05-10T10:02:32.932840Z","shell.execute_reply":"2023-05-10T10:02:34.586345Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pinecone\npinecone.init(\n    api_key=\"2832266e-7ea5-473a-b641-1afd353ff435\",\n    environment=\"us-west1-gcp-free\"\n)\n\n#This code initializes the Pinecone client by calling the init() function from the pinecone package.\n\n#The api_key parameter is set to a string value that represents the API key for your Pinecone account. This API key is used to authenticate your requests to the Pinecone service.\n\n#The environment parameter is set to a string value that represents the name of the environment where your Pinecone index will be created. In this case, it is set to \"us-west1-gcp-free\", which is a free tier environment provided by Google Cloud Platform.\n\n#Once initialized, you can use the Pinecone client to create an index and add vectors to it, as well as query the index and retrieve similar vectors.","metadata":{"_uuid":"e0cf0b7c-8b57-48c3-9700-7290c954f167","_cell_guid":"6b9a4c8a-4c70-4c94-9c71-0ef17a6722cc","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:02:38.438553Z","iopub.execute_input":"2023-05-10T10:02:38.439581Z","iopub.status.idle":"2023-05-10T10:02:38.770319Z","shell.execute_reply.started":"2023-05-10T10:02:38.439541Z","shell.execute_reply":"2023-05-10T10:02:38.769273Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_name = \"extractive-question-answering\"\n\n\nif index_name not in pinecone.list_indexes():\n\n    pinecone.create_index(\n        index_name,\n        dimension=384,\n        metric=\"cosine\"\n    )\nindex = pinecone.Index(index_name)\n\n#This code creates a Pinecone index for extractive question answering.\n\n#First, it sets the index_name variable to a string value that represents the name of the index.\n\n#Next, it checks if the index already exists by calling pinecone.list_indexes(). If the index does not exist, it creates a new index using pinecone.create_index(). The dimension parameter is set to 384, which represents the dimensionality of the vectors that will be added to the index. The metric parameter is set to \"cosine\", which specifies that cosine similarity will be used to measure distances between vectors in the index.\n\n#Finally, it creates an instance of the Pinecone Index class for this index using pinecone.Index(), and assigns it to a variable named index. This instance can be used to add vectors to the index and query its contents.","metadata":{"_uuid":"3d1b3093-dc36-4de1-b0d8-c4863f154d54","_cell_guid":"19cd2d54-2ee0-4cff-b341-b52d33b236f5","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:02:41.325326Z","iopub.execute_input":"2023-05-10T10:02:41.326047Z","iopub.status.idle":"2023-05-10T10:02:43.573612Z","shell.execute_reply.started":"2023-05-10T10:02:41.326012Z","shell.execute_reply":"2023-05-10T10:02:43.572664Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom sentence_transformers import SentenceTransformer\n\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nretriever = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=device)\nretriever\n\n# This code imports the torch package and the SentenceTransformer class from the sentence_transformers package.\n\n# The device variable is set to 0 if a GPU is available, otherwise it is set to -1. This will determine whether or not we use a GPU for computing sentence embeddings.\n\n# Next, an instance of the SentenceTransformer class is created with the model name 'multi-qa-MiniLM-L6-cos-v1'. This model has been pre-trained on a variety of NLP tasks and can be used to generate sentence embeddings. The device parameter specifies whether to use a GPU or CPU for computation.\n\n# Finally, the instance of the SentenceTransformer class is assigned to a variable named retriever. This instance can be used to generate embeddings for sentences or paragraphs.","metadata":{"_uuid":"e1a13bc2-2544-4f8d-b488-8c884fe87ce4","_cell_guid":"6cfd8499-c0ac-4812-973f-03db6daccf49","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:02:45.204247Z","iopub.execute_input":"2023-05-10T10:02:45.204634Z","iopub.status.idle":"2023-05-10T10:02:50.834478Z","shell.execute_reply.started":"2023-05-10T10:02:45.204600Z","shell.execute_reply":"2023-05-10T10:02:50.833463Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\n\nbatch_size = 64\n\nfor i in tqdm(range(0, len(df), batch_size)):\n    \n    i_end = min(i+batch_size, len(df))\n    batch = df.iloc[i:i_end]\n    emb = retriever.encode(batch[\"context\"].tolist()).tolist()\n    meta = batch.to_dict(orient=\"records\")\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    to_upsert = list(zip(ids, emb, meta))\n    _ = index.upsert(vectors=to_upsert)\n\nindex.describe_index_stats()\n\n\n# First, it imports the tqdm package for progress tracking during iteration.\n\n# The batch_size variable is set to 64, which represents the number of sentences that will be processed in each batch.\n\n# Next, a for loop is used to iterate through the rows of the DataFrame in batches. The tqdm() function is used to track progress during iteration.\n\n# Inside the loop, a batch of sentences is extracted from the DataFrame using .iloc[i:i_end], where i and i_end represent the start and end indices of each batch.\n\n# The retriever.encode() function is then called on this batch of sentences to generate their embeddings, which are converted to a list using .tolist().\n\n# The metadata associated with each sentence (i.e., its title) is extracted using .to_dict(orient=\"records\"), and assigned to a variable named meta.\n\n# A list of unique IDs is generated using a list comprehension, and assigned to a variable named ids.\n\n# Finally, these IDs, embeddings, and metadata are combined into a list of tuples using zip(), and passed as an argument to the .upsert() method of the Pinecone index instance created earlier. This method inserts or updates vectors in bulk.\n\n# After all vectors have been added to the index, the .describe_index_stats() method is called on the index instance to print out some statistics about its contents.","metadata":{"_uuid":"ae392c18-5f36-484e-b85f-93165ccb0272","_cell_guid":"6c2551af-8201-4602-807a-6414e04773f4","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:02:53.953655Z","iopub.execute_input":"2023-05-10T10:02:53.954033Z","iopub.status.idle":"2023-05-10T10:04:21.186111Z","shell.execute_reply.started":"2023-05-10T10:02:53.954000Z","shell.execute_reply":"2023-05-10T10:04:21.185094Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nmodel_name = \"deepset/electra-base-squad2\"\nreader = pipeline(tokenizer=model_name, model=model_name, task=\"question-answering\", device=device)\n\n# This code imports the pipeline function from the transformers package. This function is used to create a pipeline for a given NLP task, which can be used to process text data.\n\n# The model_name variable is set to \"deepset/electra-base-squad2\", which represents a pre-trained Electra model fine-tuned on the SQuAD 2.0 dataset. This model can be used for question answering tasks.\n\n# An instance of the pipeline is created by calling pipeline() with several arguments: the tokenizer name and model name, both set to model_name, the task parameter set to \"question-answering\", and the device parameter set to the value of the device variable (0 if GPU is available, -1 otherwise).\n\n# Finally, this instance is assigned to a variable named reader. This instance can be used to perform question answering on text data.","metadata":{"_uuid":"c52e9b7a-cfa5-4adf-b40e-7f8d2f216dc3","_cell_guid":"45cbb450-ca74-472c-a433-5faf80992535","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:04:57.571873Z","iopub.execute_input":"2023-05-10T10:04:57.572546Z","iopub.status.idle":"2023-05-10T10:05:13.169769Z","shell.execute_reply.started":"2023-05-10T10:04:57.572507Z","shell.execute_reply":"2023-05-10T10:05:13.168799Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_context(question, top_k):\n    xq = retriever.encode([question]).tolist()\n    xc = index.query(xq, top_k=top_k, include_metadata=True)\n    c = [x[\"metadata\"][\"context\"] for x in xc[\"matches\"]]\n    return c\n\nquestion = \"What is the capital of France?\"\ncontext = get_context(question, top_k = 1)\ncontext\n\n# This code defines a function get_context() that takes a question and the number of top matches to return as input.\n\n# Inside the function, the retriever.encode() function is called on the question to generate its embedding. This embedding is then used to query the Pinecone index using .query(), with top_k and include_metadata parameters set to top_k and True, respectively. This returns a dictionary containing information about the top k matches.\n\n# The metadata associated with each match (i.e., its context) is extracted from this dictionary using a list comprehension, and assigned to a variable named c.\n\n# Finally, this list of contexts is returned as output.\n\n# Outside of the function, a sample question is defined in the variable question. The get_context() function is then called with this question and a value of 1 for top_k, and the resulting context(s) are assigned to a variable named context.","metadata":{"_uuid":"9f23abc3-9e53-4a90-9078-3c33153ebb32","_cell_guid":"92f505f9-adf4-414c-ae96-d1888102dcad","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:06:28.791417Z","iopub.execute_input":"2023-05-10T10:06:28.792078Z","iopub.status.idle":"2023-05-10T10:06:28.859331Z","shell.execute_reply.started":"2023-05-10T10:06:28.792040Z","shell.execute_reply":"2023-05-10T10:06:28.858452Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\n\n\ndef extract_answer(question, context):\n    results = []\n    for c in context:\n        \n        answer = reader(question=question, context=c)\n        \n        answer[\"context\"] = c\n        results.append(answer)\n    \n    sorted_result = pprint(sorted(results, key=lambda x: x[\"score\"], reverse=True))\n    return sorted_result\n\nextract_answer(question, context)\n\n# This code defines a function named extract_answer() that takes a question and a list of contexts as input.\n\n# Inside the function, an empty list named results is created to store the answers for each context. Then, a for loop is used to iterate through each context in the input list.\n\n# For each context, the reader() function is called with the input question and context as arguments. This function uses the pre-trained Electra model fine-tuned on SQuAD 2.0 dataset to extract an answer from the given context. The extracted answer along with its score are stored in a dictionary and appended to the results list.\n\n# After all contexts have been processed, the results list is sorted in descending order of score using a lambda function and printed using pprint().\n\n# Finally, this sorted result is returned from the function.\n\n# Outside of the function, the previously defined variables question and context are passed as arguments to extract_answer(). This will return a sorted list of answers along with their scores for each context in context.","metadata":{"_uuid":"d4cdaf77-6298-4545-97aa-947b01d43238","_cell_guid":"46078fe3-27c1-4d94-8812-a24271823783","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:07:33.507515Z","iopub.execute_input":"2023-05-10T10:07:33.507921Z","iopub.status.idle":"2023-05-10T10:07:33.539266Z","shell.execute_reply.started":"2023-05-10T10:07:33.507887Z","shell.execute_reply":"2023-05-10T10:07:33.538189Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = \"What is the highest mountain in the world?\"\ncontext = get_context(question, top_k=1)\nextract_answer(question, context)","metadata":{"_uuid":"cf5d78d5-e9b2-425b-98de-097b198e95f9","_cell_guid":"e44534f5-eeef-4181-bcb1-e561f19ba6db","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:08:37.958546Z","iopub.execute_input":"2023-05-10T10:08:37.959180Z","iopub.status.idle":"2023-05-10T10:08:38.088227Z","shell.execute_reply.started":"2023-05-10T10:08:37.959141Z","shell.execute_reply":"2023-05-10T10:08:38.087007Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = \"What is the currency of Mexico?\"\ncontext = get_context(question, top_k=1)\nextract_answer(question, context)","metadata":{"_uuid":"f77d901a-9611-4044-90a2-de05da88f597","_cell_guid":"9b582ff1-baaf-4ad1-95f8-d1d6906b0d71","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:09:40.597561Z","iopub.execute_input":"2023-05-10T10:09:40.598504Z","iopub.status.idle":"2023-05-10T10:09:40.661462Z","shell.execute_reply.started":"2023-05-10T10:09:40.598458Z","shell.execute_reply":"2023-05-10T10:09:40.660316Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = question = \"Who wrote the novel To Kill a Mockingbird?\"\ncontext = get_context(question, top_k=3)\nextract_answer(question, context)","metadata":{"_uuid":"84ffe6db-7e2a-433b-a370-1d62cf17b6d3","_cell_guid":"f51a4aff-7a09-46e0-9008-3dbd05d1074b","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:10:46.137052Z","iopub.execute_input":"2023-05-10T10:10:46.137470Z","iopub.status.idle":"2023-05-10T10:10:46.276704Z","shell.execute_reply.started":"2023-05-10T10:10:46.137414Z","shell.execute_reply":"2023-05-10T10:10:46.275492Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install streamlit\n\n# Streamlit is installed for generation of frontend.","metadata":{"_uuid":"e53f4905-daa8-415c-87ab-0d1ac0ea5744","_cell_guid":"dff0cc4e-cd75-43d7-a4d2-eaf09bc0a98c","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:13:44.023751Z","iopub.execute_input":"2023-05-10T10:13:44.024164Z","iopub.status.idle":"2023-05-10T10:13:58.765771Z","shell.execute_reply.started":"2023-05-10T10:13:44.024129Z","shell.execute_reply":"2023-05-10T10:13:58.764346Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import required libraries\nimport streamlit as st\nimport pinecone\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom kaggle_secrets import UserSecretsClient\n\n# Get Pinecone API key from Kaggle secrets\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"PINECONE_KEY\")\n\n# Define function to initialize Pinecone index and return it as a Streamlit session state variable\n@st.experimental_singleton\ndef init_pinecone():\n    # Initialize Pinecone with API key and environment settings\n    pinecone.init(api_key=secret_value_0, environment=\"us-west1-gcp-free\")\n    # Return initialized index object\n    return pinecone.Index(\"extractive-question-answering\")\n    \n# Define function to initialize SentenceTransformer and Transformers models and return them as a tuple as a Streamlit session state variable \n@st.experimental_singleton\ndef init_models():\n    # Initialize SentenceTransformer model for semantic search using multi-qa-MiniLM-L6-cos-v1 model \n    retriever = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n    # Define name of Transformers model for question answering \n    model_name = 'deepset/electra-base-squad2'\n    # Initialize Transformers model for question answering using defined model name \n    reader = pipeline(tokenizer=model_name, model=model_name, task='question-answering')\n    # Return initialized models as tuple \n    return retriever, reader\n\n# Initialize Pinecone index and models as Streamlit session state variables \nst.session_state.index = init_pinecone()\nretriever, reader = init_models()\n\n# Define function to create cards with search results \ndef card(title, context, score):\n    return st.markdown(f\"\"\"\n    <div class=\"container-fluid\">\n        <div class=\"row align-items-start\">\n             <div  class=\"col-md-12 col-sm-12\">\n                 <b>{title}</b>\n                 <br>\n                 <span style=\"color: #808080;\">\n                     <small>{context}</small>\n                     [<b>Score: </b>{score}]\n                 </span>\n             </div>\n        </div>\n     </div>\n        \"\"\", unsafe_allow_html=True)\n\n# Set Streamlit app title and description \nst.title(\"\")\nst.write(\"\"\"\n# VectorFinder\n\nDiscover the power of semantic search with VectorFinder\n\n\nCreated By : Gourav Mohanty\n\"\"\")\n\n# Add Bootstrap CSS stylesheet to app \nst.markdown(\"\"\"\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css\" integrity=\"sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm\" crossorigin=\"anonymous\">\n\"\"\", unsafe_allow_html=True)\n\n# Define function to run search query and display results \ndef run_query(query):\n    # Encode query using SentenceTransformer model \n    xq = retriever.encode([query]).tolist()\n    try:\n        # Query Pinecone index with encoded query and get top 3 matches along with metadata \n        xc = st.session_state.index.query(xq, top_k=3, include_metadata=True)\n    except:\n        # If error occurs while querying Pinecone index, force reload of index \n        pinecone.init(api_key=secret_value_0, environment=\"eu-west4-gcp\")\n        st.session_state.index = pinecone.Index(\"extractive-question-answering\")\n        xc = st.session_state.index.query(xq, top_k=3, include_metadata=True)\n\n    # Initialize empty list to store search results \n    results = []\n    for match in xc['matches']:\n        # Use Transformers model to get answer to query from context \n        answer = reader(question=query, context=match[\"metadata\"]['context'])\n        # Append metadata and answer to search results list \n        answer[\"title\"] = match[\"metadata\"]['title']\n        answer[\"context\"] = match[\"metadata\"]['context']\n        results.append(answer)\n\n    # Sort search results by score in descending order \n    sorted_result = sorted(results, key=lambda x: x['score'], reverse=True)\n\n    # Create card for each search result and display it on the app \n    for r in sorted_result:\n        answer = r[\"answer\"]\n        context = r[\"context\"].replace(answer, f\"<mark>{answer}</mark>\")\n        title = r[\"title\"].replace(\"_\", \" \")\n        score = round(r[\"score\"], 4)\n        card(title, context, score)\n\n# Get user input query from text input field on app \nquery = st.text_input(\"Search!\", \"\")\n\n# If user has entered a query, run the query and display the search results on the app using cards \nif query != \"\":\n    run_query(query)\n","metadata":{"_uuid":"c842525c-40e4-4c91-9a13-7107aba059a9","_cell_guid":"ea24c1a3-c20a-4e44-9381-fc1f6b32b320","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T11:30:32.240652Z","iopub.execute_input":"2023-05-10T11:30:32.241686Z","iopub.status.idle":"2023-05-10T11:30:32.250166Z","shell.execute_reply.started":"2023-05-10T11:30:32.241644Z","shell.execute_reply":"2023-05-10T11:30:32.249147Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n    \n# Installing ngrok","metadata":{"_uuid":"457a2db2-4169-4023-97bc-4c01aa33c94f","_cell_guid":"3640c795-5ceb-4dc4-ad66-28644fd8818a","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:18:51.561551Z","iopub.execute_input":"2023-05-10T10:18:51.561976Z","iopub.status.idle":"2023-05-10T10:18:58.141511Z","shell.execute_reply.started":"2023-05-10T10:18:51.561943Z","shell.execute_reply":"2023-05-10T10:18:58.140356Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip ngrok-stable-linux-amd64.zip","metadata":{"_uuid":"2d4c3dc9-984e-44ca-b74b-1c2ab32eb028","_cell_guid":"69d45855-a22d-4c6d-9fcb-26f160df5476","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:19:10.468473Z","iopub.execute_input":"2023-05-10T10:19:10.469591Z","iopub.status.idle":"2023-05-10T10:19:11.747873Z","shell.execute_reply.started":"2023-05-10T10:19:10.469536Z","shell.execute_reply":"2023-05-10T10:19:11.746634Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_ipython().system_raw('./ngrok http 8501 &')","metadata":{"_uuid":"f1948862-2315-4c97-89c7-9e1be7ca8c58","_cell_guid":"5c7b2079-4cdf-473c-991d-d809189e5d55","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T11:30:35.586882Z","iopub.execute_input":"2023-05-10T11:30:35.587308Z","iopub.status.idle":"2023-05-10T11:30:35.601281Z","shell.execute_reply.started":"2023-05-10T11:30:35.587273Z","shell.execute_reply":"2023-05-10T11:30:35.599971Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    'import sys, json; print(\"Execute the next cell and the go to the following URL: \" +json.load(sys.stdin)[\"tunnels\"][0][\"public_url\"])'","metadata":{"_uuid":"7f660df0-2281-4129-bfac-26943c89da44","_cell_guid":"2c59c404-aa5b-4a9a-bcfc-66e45640b2d7","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T11:30:37.199385Z","iopub.execute_input":"2023-05-10T11:30:37.199794Z","iopub.status.idle":"2023-05-10T11:30:38.561244Z","shell.execute_reply.started":"2023-05-10T11:30:37.199759Z","shell.execute_reply":"2023-05-10T11:30:38.559860Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import streamlit as st\nfrom pyngrok import ngrok\n\n# Set ngrok auth token (only needed once)\nngrok.set_auth_token(\"2PYs1fPvxkScH9oIANIZXTgjHG3_5rQEz6RwxWtaSkNozeZfv\")\n\n# Start ngrok tunnel\n# public_url = ngrok.connect(addr=\"http://localhost:4040\", proto=\"http\", options={\"bind_tls\": True}).public_url\n\n\n# Run Streamlit app\n# if __name__ == '__main__':\n#     main()","metadata":{"_uuid":"a5ed3560-7e2e-4c22-9b09-85f9d8300ec7","_cell_guid":"bb945a09-98f0-4ed6-a3b5-0c6fafc9fe66","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T10:34:39.182728Z","iopub.execute_input":"2023-05-10T10:34:39.183702Z","iopub.status.idle":"2023-05-10T10:34:39.211699Z","shell.execute_reply.started":"2023-05-10T10:34:39.183649Z","shell.execute_reply":"2023-05-10T10:34:39.210562Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!streamlit run ./my_app.py\n\n# Running Streamlit application","metadata":{"_uuid":"5dcdcdd2-c722-4673-94aa-ddf42df229d8","_cell_guid":"4e31cda7-5592-42c1-ace1-132ceccb4626","collapsed":false,"execution":{"iopub.status.busy":"2023-05-10T11:30:41.976059Z","iopub.execute_input":"2023-05-10T11:30:41.976518Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"521fb7b3-6b73-45c7-b6d8-39cfcf1befea","_cell_guid":"32d462b4-2bae-43bc-9f6f-7bab943c70f3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}